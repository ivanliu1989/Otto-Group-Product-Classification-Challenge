x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))]))#[,-which(names(test) %in% c("target"))])
rm(list=ls());gc()
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R')
load(file='data/target.RData')
load(file='data/raw_data_multi.RData')
trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))]))#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,];dtrain <- shuffle(dtrain)
dtest <- x[teind,]
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))])#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,];dtrain <- shuffle(dtrain)
dtest <- x[teind,]
### Set necessary parameter ###
class(dtrain)
sf <- dtrain
sf[,'id2'] <- sample(1:nrow(sf), nrow(sf), replace=T)
nrow(sf)
sample(1:nrow(sf), nrow(sf), replace=T)
sf[,'id2']
head()
head(sf)
dtrain <- x[trind,];dtrain <- shuffle(as.data.frame(dtrain))
head(dtrain)
dtrain <- x[trind,];dtrain <- data.matraix(shuffle(as.data.frame(dtrain)))
dtrain <- x[trind,];dtrain <- data.matrix(shuffle(as.data.frame(dtrain)))
head(dtrain)
class(dtrain)
dtest <- data.matrix(x[teind,])
head(dtest)
rm(list=ls());gc()
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R')
load(file='data/target.RData')
load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train)
head(train)
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
rm(list=ls());gc()
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R')
load(file='data/target.RData')
load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train)
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 8, eta=0.05, gamma = 0.05,
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# eta=0.3, gamma = 1, num_class" = 9, max.depth=10, min_child_weight=4, subsample=0.9, colsample_bytree = 0.8
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
# Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
# bst.cv$dt
# pred <- bst.cv$pred
### Train the model ###
set.seed(8)
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred1 <- pred
save(pred1, file='../xgboost_pred1.RData')
dim(pred1)
dim(test)
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 18, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(18) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 18, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(18) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred2 <- pred #<<============#
save(pred2, file='../xgboost_pred2.RData') #<<============#
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 28, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(28) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred3 <- pred #<<============#
save(pred3, file='../xgboost_pred3.RData') #<<============#
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 38, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(38) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred4 <- pred #<<============#
save(pred4, file='../xgboost_pred4.RData') #<<============#
# pred_ensemble <- (pred1 + pred2)/2
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 48, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(48) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred5 <- pred #<<============#
save(pred5, file='../xgboost_pred5.RData')
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 58, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(58) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred6 <- pred #<<============#
save(pred6, file='../xgboost_pred6.RData')
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 68, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(68) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred7 <- pred #<<============#
save(pred7, file='../xgboost_pred7.RData')
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 78, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(78) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred8 <- pred #<<============#
save(pred8, file='../xgboost_pred8.RData')
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 88, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(88) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred9 <- pred #<<============#
save(pred9, file='../xgboost_pred9.RData')
require(caret);require(methods);require(xgboost)
source('main/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_multi.RData')
# trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
# train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- shuffle(train) #<<============#
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test)#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 2, set.seed = 98, eta=0.05, gamma = 0.05, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=1,
subsample=0.8, colsample_bytree = 0.9)
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 668
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(98) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
pred10 <- pred #<<============#
save(pred10, file='../xgboost_pred10.RData')
pred_ensemble <- (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10)/10
pred_ensemble = format(pred_ensemble, digits=2,scientific=F) # shrink the size of submission
pred_ensemble = data.frame(1:nrow(pred_ensemble),pred_ensemble)
names(pred_ensemble) = c('id', paste0('Class_',1:9))
write.csv(pred_ensemble,file='submission_avg_10.csv', quote=FALSE,row.names=FALSE)
for (i in 1:9){
for (j in 1:nrow(pred1)){
pred_ensemble[j,i] <- max(pred1[j,i],pred2[j,i],pred3[j,i],pred4[j,i],pred5[j,i],pred6[j,i]
,pred7[j,i],pred8[j,i],pred9[j,i],pred10[j,i])
}
}
class(pred_ensemble)
pred_ensemble <- (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10)/10
class(pred_ensemble)
for (i in 1:9){
for (j in 1:nrow(pred1)){
pred_ensemble[j,i] <- max(pred1[j,i],pred2[j,i],pred3[j,i],pred4[j,i],pred5[j,i],pred6[j,i]
,pred7[j,i],pred8[j,i],pred9[j,i],pred10[j,i])
}
}
pred_ensemble = format(pred_ensemble, digits=2,scientific=F) # shrink the size of submission
pred_ensemble = data.frame(1:nrow(pred_ensemble),pred_ensemble)
names(pred_ensemble) = c('id', paste0('Class_',1:9))
write.csv(pred_ensemble,file='submission_max_10.csv', quote=FALSE,row.names=FALSE)
