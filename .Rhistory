pred = t(pred)
target_df <- target[-trainIndex,]
MulLogLoss(target_df,pred)
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.6, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
target_df <- target[-trainIndex,]
MulLogLoss(target_df,pred)
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.8, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
target_df <- target[-trainIndex,]
MulLogLoss(target_df,pred)
seeds <- 268
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
target_df <- target[-trainIndex,]
MulLogLoss(target_df,pred)
train <- train_tot[,1:95]; target <- train_tot[,96:104]
trainIndex <- createDataPartition(train$target, p = .95,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))])#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
seeds <- 268
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
train <- train_tot[,1:95]; target <- train_tot[,96:104]
trainIndex <- createDataPartition(train$target, p = .95,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))])#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
dim(x)
train <- train_tot[,1:95]; target <- train_tot[,96:104]
trainIndex <- createDataPartition(train$target, p = .95,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
dim(train_df)
dim(test_df)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- train_tot[,1:95]; target <- train_tot[,96:104]
trainIndex <- createDataPartition(train$target, p = .95,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train = train_df[,-which(names(train_tot) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))])#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
teind
nrow(train)+1
train <- train_tot[,1:95]; target <- train_tot[,96:104]
trainIndex <- createDataPartition(train$target, p = .95,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
train <- train_tot[,1:95]; target <- train_tot[,96:104]
trainIndex <- createDataPartition(train$target, p = .95,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))])#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
seeds <- 268
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
target_df <- target[-trainIndex,]
MulLogLoss(target_df,pred)
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
seeds <- 268
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
bst = xgboost(param=param, data = train, label = y, nround = cv.nround)
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
dtrain <- as.matrix(train)
seeds <- 268
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
train <- train_tot[,1:95]; target <- train_tot[,96:104]
test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
library(doMC)
registerDoMC(cores = 3)
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
head(train[,-c('target')])
head(train[,-94])
head(train[])
head(train[,-94])
dtrain <- as.matrix(train[,-94])
seeds <- 268
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
head(dtrain)
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
as.numeric(train)
dtrain = matrix(as.numeric(as.matrix(train)),nrow(train),ncol(train))
train <- as.matrix(train)
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
train <- as.matrix(train)
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
train <- as.matrix(train)
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
train <- as.matrix(train[,-94])
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
head(dtrain)
seeds <- 268
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
head(test)
test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
head(test)
test <- as.matrix(test[,-1])
dtest = matrix(as.numeric(test),nrow(test),ncol(test))
pred = predict(bst,dtest)#, ntreelimit=1
i <- 31
write.csv(pred,file=paste0('../xgboost/xgboost_', i, '.csv'), quote=FALSE,row.names=FALSE)
head(pred)
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(1:nrow(pred),pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../xgboost/xgboost_', i, '.csv'), quote=FALSE,row.names=FALSE)
head(pred)
for (i in 1:30){
seeds <- 8 * i
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(1:nrow(pred),pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../xgboost/xgboost_', i, '.csv'), quote=FALSE,row.names=FALSE)
print(paste0('Model:',i,' Complete!'))
}
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');
train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
library(doMC)
registerDoMC(cores = 4)
trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train_df <- shuffle(train_df) #<<============#
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))])#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
# param <- list("objective" = "multi:softprob",
#               "eval_metric" = "mlogloss",
#               "nthread" = 3, set.seed = 168, eta=0.05, gamma = 0.2, #<<============#
#               "num_class" = 9, max.depth=, min_child_weight=6,
#               subsample=0.8, colsample_bytree = 0.9)
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = 168, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=6, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 698
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(168) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
# pred17 <- pred #<<============#
# save(pred17, file='../xgboost_pred17.RData') #<<============#
# pred_ensemble <- (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10)/10
# for (i in 1:9){
#     for (j in 1:nrow(pred1)){
#         pred_ensemble[j,i] <- max(pred1[j,i],pred2[j,i],pred3[j,i],pred4[j,i],pred5[j,i],pred6[j,i]
#                                   ,pred7[j,i],pred8[j,i],pred9[j,i],pred10[j,i],pred11[j,i],pred12[j,i],pred13[j,i],pred14[j,i],
#                                   pred15[j,i],pred16[j,i],pred17[j,i])
#     }
# }
### Validation ###
target_df <- target[-trainIndex,]
MulLogLoss(target_df,pred)
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = 168, eta=0.05, gamma = 0.03, #0.05
"num_class" = 9, max.depth=6, min_child_weight=4,
subsample=0.7, colsample_bytree = 0.6) #0.8
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 758
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(168) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
# pred17 <- pred #<<============#
# save(pred17, file='../xgboost_pred17.RData') #<<============#
# pred_ensemble <- (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10)/10
# for (i in 1:9){
#     for (j in 1:nrow(pred1)){
#         pred_ensemble[j,i] <- max(pred1[j,i],pred2[j,i],pred3[j,i],pred4[j,i],pred5[j,i],pred6[j,i]
#                                   ,pred7[j,i],pred8[j,i],pred9[j,i],pred10[j,i],pred11[j,i],pred12[j,i],pred13[j,i],pred14[j,i],
#                                   pred15[j,i],pred16[j,i],pred17[j,i])
#     }
# }
### Validation ###
target_df <- target[-trainIndex,]
MulLogLoss(target_df,pred)
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_log.RData');
ls()
head(train)
library(doMC)
registerDoMC(cores = 3)
trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train_df <- shuffle(train_df) #<<============#
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))])#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = 168, eta=0.05, gamma = 0.2, #<<============#
"num_class" = 9, max.depth=, min_child_weight=6,
subsample=0.8, colsample_bytree = 0.9)
# param <- list("objective" = "multi:softprob",
#               "eval_metric" = "mlogloss",
#               "nthread" = 3, set.seed = 168, eta=0.05, gamma = 0.03, #0.05
#               "num_class" = 9, max.depth=6, min_child_weight=4,
#               subsample=0.7, colsample_bytree = 0.6) #0.8
# max.depth = 8, eta = 0.05, nround = 668, gamma = 0.05, subsample=0.8, colsample_bytree = 0.9
# reg:logistic | logloss | lambda = 0 (L2) | alpha = 0 (L1) | lambda_bias = 0
### Run Cross Valication
cv.nround = 758
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(168) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
# pred17 <- pred #<<============#
# save(pred17, file='../xgboost_pred17.RData') #<<============#
# pred_ensemble <- (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10)/10
# for (i in 1:9){
#     for (j in 1:nrow(pred1)){
#         pred_ensemble[j,i] <- max(pred1[j,i],pred2[j,i],pred3[j,i],pred4[j,i],pred5[j,i],pred6[j,i]
#                                   ,pred7[j,i],pred8[j,i],pred9[j,i],pred10[j,i],pred11[j,i],pred12[j,i],pred13[j,i],pred14[j,i],
#                                   pred15[j,i],pred16[j,i],pred17[j,i])
#     }
# }
### Validation ###
target_df <- target[-trainIndex,]
MulLogLoss(target_df,pred)
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_log.RData');
library(doMC)
registerDoMC(cores = 3)
trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train_df <- shuffle(train_df) #<<============#
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))])#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = 168, eta=0.05, gamma = 0.2, #<<============#
"num_class" = 9, max.depth=, min_child_weight=6,
subsample=0.8, colsample_bytree = 0.9)
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = 168, eta=0.05, gamma = 0.2, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=6,
subsample=0.8, colsample_bytree = 0.9)
cv.nround = 698
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(168) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
### Ensemble ###
# pred17 <- pred #<<============#
# save(pred17, file='../xgboost_pred17.RData') #<<============#
# pred_ensemble <- (pred1 + pred2 + pred3 + pred4 + pred5 + pred6 + pred7 + pred8 + pred9 + pred10)/10
# for (i in 1:9){
#     for (j in 1:nrow(pred1)){
#         pred_ensemble[j,i] <- max(pred1[j,i],pred2[j,i],pred3[j,i],pred4[j,i],pred5[j,i],pred6[j,i]
#                                   ,pred7[j,i],pred8[j,i],pred9[j,i],pred10[j,i],pred11[j,i],pred12[j,i],pred13[j,i],pred14[j,i],
#                                   pred15[j,i],pred16[j,i],pred17[j,i])
#     }
# }
### Validation ###
target_df <- target[-trainIndex,]
MulLogLoss(target_df,pred)
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');
load(file='data/raw_data_log.RData');
library(doMC)
registerDoMC(cores = 3)
