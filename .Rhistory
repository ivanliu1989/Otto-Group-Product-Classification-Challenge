train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
train <- as.matrix(train[,-94])
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
test <- as.matrix(test[,-1])
dtest = matrix(as.numeric(test),nrow(test),ncol(test))
for (i in 1:30){
seeds <- 8 * i
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(1:nrow(pred),pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../xgboost_2/xgboost_', i, '.csv'), quote=FALSE,row.names=FALSE)
print(paste0('Model:',i,' Complete!'))
}
folds <- fread('data/train_folds.csv', header=T, stringsAsFactor = F, data.table=F)
folds
folds <- fread('data/train_folds.csv', header=T, stringsAsFactor = F, data.table=F)$test_fold
train_tot <- cbind(train, target, folds)
train_tot[1:10,]
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');
load(file='data/raw_data_log.RData');
# train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
# test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
folds <- fread('data/train_folds.csv', header=T, stringsAsFactor = F, data.table=F)$test_fold
library(doMC)
registerDoMC(cores = 3)
train_tot <- cbind(train, target, folds)
head(train_tot)
targey
target
head(train_tot)
train_tot <- shuffle(train_tot) #<<============#
train <- train_tot[,1:95]; target <- train_tot[,96:104]; folds <- train[,105]
train <- train_tot[,1:95]; target <- train_tot[,96:104]; folds <- train_tot[,105]
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
train_test <- train[which(folds == 0),];train_train <- train[!which(folds == 0),]
head(train_test)
head(train_train)
train_test <- train[which(folds == 0),];train_train <- train[which(folds != 0),]
head(train_train)
dim(train_train)
dim(train_test)
12378/(12378+49500)
trainIndex <- which(folds != 0)
head(trainIndex)
head(trainIndex,20)
trainIndex <- which(folds != 0)
target_df <- target[-trainIndex,]
target_df
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');
load(file='data/raw_data_log.RData');
# train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
# test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
folds <- fread('data/train_folds.csv', header=T, stringsAsFactor = F, data.table=F)$test_fold
library(doMC)
registerDoMC(cores = 3)
trainIndex <- which(folds != 0)
target_df <- target[-trainIndex,]
target_df
train_tot <- cbind(train, target, folds)
train_tot <- shuffle(train_tot) #<<============#
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');
load(file='data/raw_data_log.RData');
# train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
# test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
folds <- fread('data/train_folds.csv', header=T, stringsAsFactor = F, data.table=F)$test_fold
library(doMC)
registerDoMC(cores = 3)
trainIndex <- which(folds != 0)
target_df <- target[-trainIndex,]
train_test <- train[which(folds == 0),];train_train <- train[which(folds != 0),]
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');
load(file='data/raw_data_log.RData');
# train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
# test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
folds <- fread('data/train_folds.csv', header=T, stringsAsFactor = F, data.table=F)$test_fold
library(doMC)
registerDoMC(cores = 3)
trainIndex <- which(folds != 0)
target_df <- target[-trainIndex,];target_train <- target[trainIndex,]
train_test <- train[which(folds == 0),];train_train <- train[which(folds != 0),]
train_tot <- cbind(train_train, target_train)
train_tot <- shuffle(train_tot) #<<============#
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
test = test[,-which(names(test) %in% c("id"))] #test
train_test = train_test[,-which(names(train_test) %in% c("id"))] #train_test
head(train)
head(train_test)
train_test
head(train)
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
train <- as.matrix(train[,-94])
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
test <- as.matrix(test[,-1])
dtest = matrix(as.numeric(test),nrow(test),ncol(test))
head(dtest)
head(test)
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');
load(file='data/raw_data_log.RData');
# train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
# test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
folds <- fread('data/train_folds.csv', header=T, stringsAsFactor = F, data.table=F)$test_fold
library(doMC)
registerDoMC(cores = 3)
trainIndex <- which(folds != 0)
target_df <- target[-trainIndex,];target_train <- target[trainIndex,]
train_test <- train[which(folds == 0),];train_train <- train[which(folds != 0),]
train_tot <- cbind(train_train, target_train)
train_tot <- shuffle(train_tot) #<<============#
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
train_test = train_test[,-which(names(train_test) %in% c("id"))] #train_test
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
train <- as.matrix(train[,-94])
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
train_test <- as.matrix(train_test[,-94])
dtrain_test = matrix(as.numeric(train_test),nrow(train_test),ncol(train_test))
test <- as.matrix(test)
dtest = matrix(as.numeric(test),nrow(test),ncol(test))
dim(test)
dim(train)
dim(train_test)
i=31
seeds <- 8 * i
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
pred = predict(bst,train_test)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
score <- MulLogLoss(target_df,pred)
score
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(trainIndex,pred)
names(pred) = c('id', paste0('Class_',1:9))
paste0('../Team_xgb/Val/valPred_Ivan_xgb_',i,'_',score,'.csv'
)
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');
load(file='data/raw_data_log.RData');
# train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
# test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
folds <- fread('data/train_folds.csv', header=T, stringsAsFactor = F, data.table=F)$test_fold
library(doMC)
registerDoMC(cores = 3)
trainIndex <- which(folds == 0)
target_df <- target[trainIndex,];target_train <- target[-trainIndex,]
train_test <- train[which(folds == 0),];train_train <- train[which(folds != 0),]
train_tot <- cbind(train_train, target_train)
train_tot <- shuffle(train_tot) #<<============#
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
train_test = train_test[,-which(names(train_test) %in% c("id"))] #train_test
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
train <- as.matrix(train[,-94])
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
train_test <- as.matrix(train_test[,-94])
dtrain_test = matrix(as.numeric(train_test),nrow(train_test),ncol(train_test))
test <- as.matrix(test)
dtest = matrix(as.numeric(test),nrow(test),ncol(test))
for (i in 1:30){
seeds <- 8 * i
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Pred Validation ###
pred = predict(bst,train_test)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
score <- MulLogLoss(target_df,pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(trainIndex,pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../Team_xgb/Val/valPred_Ivan_xgb_',i,'_',score,'.csv'),
quote=FALSE,row.names=FALSE)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(1:nrow(pred),pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../Team_xgb/Pred/testPred_Ivan_xgb_',i,'.csv'),
quote=FALSE,row.names=FALSE)
print(paste0('Model:',i,' Complete!'))
}
for (i in 1:30){
seeds <- 9 * i
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Pred Validation ###
pred = predict(bst,train_test)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
score <- MulLogLoss(target_df,pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(trainIndex,pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../Team_xgb/Val/valPred_Ivan_m',i,'CV',score,'_xgb.csv'),
quote=FALSE,row.names=FALSE)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(1:nrow(pred),pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../Team_xgb/Pred/testPred_Ivan_m',i,'_xgb.csv'),
quote=FALSE,row.names=FALSE)
print(paste0('Model:',i,' Complete!'))
}
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');
load(file='data/raw_data_log.RData');
# train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
# test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
folds <- fread('data/train_folds.csv', header=T, stringsAsFactor = F, data.table=F)$test_fold
library(doMC)
registerDoMC(cores = 3)
trainIndex <- which(folds == 0)
target_df <- target[trainIndex,];target_train <- target[-trainIndex,]
train_test <- train[which(folds == 0),];train_train <- train[which(folds != 0),]
train_tot <- cbind(train_train, target_train)
train_tot <- shuffle(train_tot) #<<============#
train <- train_tot[,1:95]; target <- train_tot[,96:104]
train = train[,-which(names(train) %in% c("id"))] #train
train_test = train_test[,-which(names(train_test) %in% c("id"))] #train_test
test = test[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
train <- as.matrix(train[,-94])
dtrain = matrix(as.numeric(train),nrow(train),ncol(train))
train_test <- as.matrix(train_test[,-94])
dtrain_test = matrix(as.numeric(train_test),nrow(train_test),ncol(train_test))
test <- as.matrix(test)
dtest = matrix(as.numeric(test),nrow(test),ncol(test))
for (i in 1:30){
seeds <- 9 * i
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05, gamma = 0.01, #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=0.7, colsample_bytree = 0.6) #0.8
cv.nround = 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Pred Validation ###
pred = predict(bst,train_test)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
score <- MulLogLoss(target_df,pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(trainIndex,pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../Team_xgb/Val/valPred_Ivan_m',i,'_CV',score,'_xgb.csv'),
quote=FALSE,row.names=FALSE)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(1:nrow(pred),pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../Team_xgb/Pred/testPred_Ivan_m',i,'_xgb.csv'),
quote=FALSE,row.names=FALSE)
print(paste0('Model:',i,' Complete!'))
}
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
require(data.table)
datadirectory <- '../Team_nnet/Val' # 'results/best'
files <- list.files(datadirectory,full.names = T)
files
dim(target_df)
ensem_prob <- matrix(0, nrow = 12378, ncol = 10, dimnames = list(NULL, NULL))
for (file in files){
result <- data.frame(fread(file,header = T, stringsAsFactor = F))
for (i in 1:10){
for (j in 1:nrow(ensem_prob)){
ensem_prob[j,i] <- max(result[j,i],ensem_prob[j,i])
}
}
}
ensem_prob[,2:10] <- prop.table(ensem_prob[,2:10], 1)
head(ensem_prob)
MulLogLoss(target_df,ensem_prob)
class(target_df)
class(ensem_prob)
MulLogLoss(target_df,ensem_prob[,2:10] )
ensem_prob <- matrix(0, nrow = 12378, ncol = 10, dimnames = list(NULL, NULL))
datadirectory <- '../Team_xgb/Val' # 'results/best'
files <- list.files(datadirectory,full.names = T)
for (file in files){
result <- data.frame(fread(file,header = T, stringsAsFactor = F))
for (i in 1:10){
for (j in 1:nrow(ensem_prob)){
ensem_prob[j,i] <- max(result[j,i],ensem_prob[j,i])
}
}
}
ensem_prob[,2:10] <- prop.table(ensem_prob[,2:10], 1)
MulLogLoss(target_df,ensem_prob[,2:10] )
datadirectory <- '../Team_xgb/Val' # 'results/best'
files <- list.files(datadirectory,full.names = T)
for (file in files){
result <- data.frame(fread(file,header = T, stringsAsFactor = F))
for (i in 1:10){
for (j in 1:nrow(ensem_prob)){
ensem_prob[j,i] <- mean(result[j,i],ensem_prob[j,i])
}
}
}
ensem_prob[,2:10] <- prop.table(ensem_prob[,2:10], 1)
MulLogLoss(target_df,ensem_prob[,2:10] )
length(files)
ensem_prob <- matrix(0, nrow = 12378, ncol = 10, dimnames = list(NULL, NULL))
datadirectory <- '../Team_xgb/Val' # 'results/best'
files <- list.files(datadirectory,full.names = T)
for (file in files){
result <- data.frame(fread(file,header = T, stringsAsFactor = F))
for (i in 1:10){
for (j in 1:nrow(ensem_prob)){
ensem_prob[j,i] <- sum(result[j,i],ensem_prob[j,i])
}
}
}
# ensem_prob[,2:10] <- prop.table(ensem_prob[,2:10], 1)
ensem_prob[,2:10] <- ensem_prob[,2:10]/length(files)
MulLogLoss(target_df,ensem_prob[,2:10] )
files[1:15]
ensem_prob <- matrix(0, nrow = 12378, ncol = 10, dimnames = list(NULL, NULL))
datadirectory <- '../Team_xgb/Val' # 'results/best'
files <- list.files(datadirectory,full.names = T)
for (file in files[1:15]){
result <- data.frame(fread(file,header = T, stringsAsFactor = F))
for (i in 1:10){
for (j in 1:nrow(ensem_prob)){
ensem_prob[j,i] <- sum(result[j,i],ensem_prob[j,i])
}
}
}
ensem_prob <- matrix(0, nrow = 12378, ncol = 10, dimnames = list(NULL, NULL))
datadirectory <- '../Team_xgb/Val' # 'results/best'
files <- list.files(datadirectory,full.names = T)
for (file in files[1:15]){
result <- data.frame(fread(file,header = T, stringsAsFactor = F))
for (i in 1:10){
for (j in 1:nrow(ensem_prob)){
ensem_prob[j,i] <- sum(result[j,i],ensem_prob[j,i])
}
}
}
# ensem_prob[,2:10] <- prop.table(ensem_prob[,2:10], 1)
ensem_prob[,2:10] <- ensem_prob[,2:10]/length(files[1:15])
MulLogLoss(target_df,ensem_prob[,2:10] )
0.7:0.9
range(0.7:0.9)
range(0.7,0.9)
sample(c(0.7,0.75,0.8),1)
sample(c(0.01,0.02,0.03,0.04,.05),1)
sample(c(690:710),1)
sample(c(690:710),1)
sample(c(690:710),1)
sample(c(690:710),1)
for (i in 1:30){
seeds <- 9 * i
set.seed(seeds) #<<============#
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = seeds, eta=0.05,
gamma = sample(c(0.01,0.02,0.03,0.04,.05),1), #0.05
"num_class" = 9, max.depth=8, min_child_weight=5,
subsample=sample(c(0.7,0.75,0.8),1), colsample_bytree = sample(c(0.6,0.7,0.9,0.8),1)) #0.8
#0.05, 0.8, 0.9 | 0.01, 0.7, 0.6
cv.nround = sample(c(690:710),1)
# 698
### Train the model ###
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
### Pred Validation ###
pred = predict(bst,train_test)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
score <- MulLogLoss(target_df,pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(trainIndex,pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../Team_xgb/Val/valPred_Ivan_m',i,'_CV',score,'_xgb2.csv'),
quote=FALSE,row.names=FALSE)
### Make prediction ###
pred = predict(bst,dtest)#, ntreelimit=1
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(1:nrow(pred),pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file=paste0('../Team_xgb/Pred/testPred_Ivan_m',i,'_xgb2.csv'),
quote=FALSE,row.names=FALSE)
print(paste0('Model:',i,' Complete!'))
}
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(caret);require(methods);require(xgboost);require(data.table)
source('main_R/2_logloss_func.R');load(file='data/target.RData');load(file='data/raw_data_log.RData');
# train <- fread('../train.csv', header=T, stringsAsFactor = F,data.table=F)
# test <- fread('../test.csv', header=T, stringsAsFactor = F, data.table=F)
library(doMC)
registerDoMC(cores = 3)
trainIndex <- createDataPartition(train$target, p = .7,list = FALSE)
train_df <- train[trainIndex,];test_df  <- train[-trainIndex,]
train_df <- shuffle(train_df) #<<============#
train = train_df[,-which(names(train) %in% c("id"))] #train
test = test_df[,-which(names(test) %in% c("id"))] #test
y = train[,'target']
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)
x = rbind(train[,-which(names(train) %in% c("target"))],test[,-which(names(test) %in% c("target"))])#[,-which(names(test) %in% c("target"))])
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)
dtrain <- x[trind,]
dtest <- data.matrix(x[teind,])
### Set necessary parameter ###
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"nthread" = 3, set.seed = 168, eta=0.05, gamma = 0.2, #<<============#
"num_class" = 9, max.depth=8, min_child_weight=6,
subsample=0.8, colsample_bytree = 0.9)
cv.nround = 698
# bst.cv = xgb.cv(param=param, data = dtrain, label = y, nfold = 10,
#                 nrounds=cv.nround,prediction = TRUE)
### Train the model ###
set.seed(168) #<<============#
bst = xgboost(param=param, data = dtrain, label = y, nround = cv.nround)
setwd('/Users/ivanliu/Google Drive/otto/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()
require(data.table)
result1 <- data.frame(fread('../xgb_blending_32.csv',header = T, stringsAsFactor = F))
head(result1)
class(result1[,1])
options(scipen=3)
class(result1[,1])
head(result1)
write.csv(result1,file='../xgb_blending_32.csv', quote=FALSE,row.names=FALSE)
