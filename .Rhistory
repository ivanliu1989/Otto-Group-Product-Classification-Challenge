# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=1e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-8)
# ,nfolds=10,adaptive_rate=0.9,, rho = 0.99, train_samples_per_iteration = -2
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=1e-8,shuffle_training_data=T,max_w2=4, epsilon = 1e-8)
# ,nfolds=10,adaptive_rate=0.9,, rho = 0.99, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-8)
# ,nfolds=10,adaptive_rate=0.9,, rho = 0.99, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-8)
# ,nfolds=10,adaptive_rate=0.9,, rho = 0.99, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=3, epsilon = 1e-8)
# ,nfolds=10,adaptive_rate=0.9,, rho = 0.99, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=1, epsilon = 1e-8)
# ,nfolds=10,adaptive_rate=0.9,, rho = 0.99, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10)
# ,nfolds=10,adaptive_rate=0.9,, rho = 0.99, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-15)
# ,nfolds=10,adaptive_rate=0.9,, rho = 0.99, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10)
# ,nfolds=10,adaptive_rate=0.9,, rho = 0.99, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.9)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 1.1)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 1)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.8)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.98)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.9,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=5,variable_importances=F,rate_decay=0.9,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=2,variable_importances=F,rate_decay=0.9,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=2,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,adaptive_rate=0.9,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0,hidden_dropout_ratios = c(0,0,0),seed=8,adaptive_rate=0.9,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0),seed=8,
hidden=c(64,64,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,20),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.1,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,20),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.5,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(64,64,20),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(100,100,20),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(100,100,50),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(100,100,6),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(100,81,9),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(100,100,20),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(100,100,30),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=list(c(100,100,20),c(100,90,20),c(100,80,20)),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
fit
pred <- h2o.predict(object = fit, newdata = test_df)
pred <- h2o.predict(object = fit[1], newdata = test_df)
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0,0,0),seed=8,
hidden=c(100,90,20),epochs=1,variable_importances=F,rate_decay=0.66,rate=0.1,
override_with_best_model=T,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l2=3e-6,shuffle_training_data=T,max_w2=4, epsilon = 1e-10, rho = 0.99)
# ,nfolds=10,, train_samples_per_iteration = -2,l1=1e-5,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df, type = "BigData",
#                         classification=T, ntree=5000, depth=30, mtries=30,
#                         sample.rate=0.8, nbins = 30, seed=8,verbose=T)
# nodesize=10, validation=
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
