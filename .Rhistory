# rate_decay=0.1,,epsilon=0.1,rate=0.1,nesterov_accelerated_gradient=F,nfolds=10,seed=8,
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,
override_with_best_model=F,loss='CrossEntropy',
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,rate_decay=0.1,
override_with_best_model=F,loss='CrossEntropy',
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,rate_decay=0.3,
override_with_best_model=F,loss='CrossEntropy',
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nesterov_accelerated_gradient=F,nfolds=10,seed=8,adaptive_rate=0.9,rate=0.1,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,rate_decay=0.5,
override_with_best_model=F,loss='CrossEntropy',
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nesterov_accelerated_gradient=F,nfolds=10,seed=8,adaptive_rate=0.9,rate=0.1,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,rate_decay=0.3,adaptive_rate=0.9,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,rate_decay=0.3,adaptive_rate=c(0.9,0.9),
override_with_best_model=F,loss='CrossEntropy',
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nesterov_accelerated_gradient=F,nfolds=10,seed=8,adaptive_rate=0.9,rate=0.1,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nesterov_accelerated_gradient=F,nfolds=10,seed=8,adaptive_rate=0.9,rate=0.1,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nesterov_accelerated_gradient=F,nfolds=10,seed=8,adaptive_rate=0.9,rate=0.1,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=F,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nesterov_accelerated_gradient=F,nfolds=10,seed=8,adaptive_rate=0.9,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nesterov_accelerated_gradient=F,nfolds=10,seed=8,adaptive_rate=0.9,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=2,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nesterov_accelerated_gradient=F,nfolds=10,seed=8,adaptive_rate=0.9,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="RectifierWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5),
hidden=c(512,512),epochs=2,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nfolds=10,seed=8,adaptive_rate=0.9,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0.5),
hidden=c(512,512,512),epochs=2,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nfolds=10,seed=8,adaptive_rate=0.9,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0.5),
hidden=c(512,512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,epsilon=0.1,nfolds=10,seed=8,adaptive_rate=0.9,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0.5),seed=8,
hidden=c(512,512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4,epsilon=0.1)
# ,nfolds=10,adaptive_rate=0.9,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0.5),seed=8,
hidden=c(512,512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4,epsilon=0.01)
# ,nfolds=10,adaptive_rate=0.9,
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0.5),seed=8,
hidden=c(512,512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,nfolds=10,adaptive_rate=0.9,,epsilon=0.01
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.6,hidden_dropout_ratios = c(0.5,0.5,0.5),seed=8,
hidden=c(512,512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,nfolds=10,adaptive_rate=0.9,,epsilon=0.01
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.2,0.2,0.2),seed=8,
hidden=c(512,512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,nfolds=10,adaptive_rate=0.9,,epsilon=0.01
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.8,0.8,0.8),seed=8,
hidden=c(512,512,512),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,nfolds=10,adaptive_rate=0.9,,epsilon=0.01
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0.5),seed=8,
hidden=c(100,100,100),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,nfolds=10,adaptive_rate=0.9,,epsilon=0.01
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0.5),seed=8,
hidden=c(1000,1000,1000),epochs=1,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,nfolds=10,adaptive_rate=0.9,,epsilon=0.01
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0.5),seed=8,
hidden=c(500,500,500),epochs=5,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,nfolds=10,adaptive_rate=0.9,,epsilon=0.01
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
classification=T,activation="TanhWithDropout",#TanhWithDropout Rectifier
input_dropout_ratio = 0.2,hidden_dropout_ratios = c(0.5,0.5,0.5),seed=8,
hidden=c(500,500,500),epochs=10,variable_importances=F,rate_decay=0.3,rate=0.1,
override_with_best_model=F,loss='CrossEntropy',nesterov_accelerated_gradient=T,
l1=3e-6, l2=6e-6,shuffle_training_data=T,max_w2=4)
# ,nfolds=10,adaptive_rate=0.9,,epsilon=0.01
# fit <- h2o.randomForest(y = dependent, x = independent, data = train_df,
#                         classification=T, ntree=500, depth=30, mtries=30,
#                         sample.rate=0.8, nbins=T, seed=8,verbose=T)
# nodesize=10,
pred <- h2o.predict(object = fit, newdata = test_df)
pred_ensemble = format(as.data.frame(pred[,2:10]), digits=2,scientific=F) # shrink the size of submission
target_df <- target[-trainIndex,]
MulLogLoss(target_df,data.matrix(pred_ensemble))
setwd('/Users/ivan/Work_directory/Otto-Group-Product-Classification-Challenge');
rm(list=ls());gc()library(caret)
rm(list=ls());gc()
load(file='data/raw_data_log.RData')
ls()
head(train)
head(train)
head(train,1)
head(train,1)
dim(train)
row <- paste0(train[1,95],"'",train[1,1],train[1,2:94])
row
train[1,95]
class(train[,95])
levels(train[,95])
levels(as.factor(train[,95]))
levels(as.factor(train[,95])) <- 1:9
levels(as.factor(train[,95])) <- c(1:9)
train[,95]<-as.factor(train[,95])
levels(train[,95]) <- c(1:9)
levels(train[,95])
head(train)
head(test)
row <- paste0(train[1,95],"'",train[1,1],train[1,2:94])
row
paste0("'",train[1,1])
names(train)
col <- names(train)
col[2]
row <- paste0(train[,95],paste0(" '",train[,1]),"|",
col[2],":",train[,2]," ",
col[3],":",train[,3]," ",
col[4],":",train[,4]," ",
col[5],":",train[,5]," ",
col[6],":",train[,6]," ",
col[7],":",train[,7]," ",
col[8],":",train[,8]," ",
col[9],":",train[,9]," ",
col[10],":",train[,10]," ",
col[11],":",train[,11]," ",
col[12],":",train[,12]," ",
col[13],":",train[,13]," ",
col[14],":",train[,14]," ",
col[15],":",train[,15]," ",
col[16],":",train[,16]," ",
col[17],":",train[,17]," ",
col[18],":",train[,18]," ",
col[19],":",train[,19]," ",
col[20],":",train[,20]," ",
col[21],":",train[,21]," ",
col[22],":",train[,22]," ",
col[23],":",train[,23]," "
)
row
head(row)
head(train)
sapply()
row <- sapply(2:94, paste0(col[i],":",train[,i]," "))
row <- sapply(2:94, function(i) {paste0(col[i],":",train[,i]," ")})
dim(row)
row <- paste0(train[,95],paste0(" '",train[,1]),"|")
row2 <- sapply(2:94, function(i) {paste0(col[i],":",train[,i]," ")})
train_vw <- cbind(row,row2)
head(train_vw)
row2 <- sapply(2:94, function(i) {paste0(col[i],":",train[,i]," ")})
head(row2)
?lapply
?mapply
?tapply
train_vw <- lapply(train_vw,2,paste0)
train_vw <- lapply(train_vw,INDEX=2,paste0)
head(train_vw)
class(train_vw)
train_vw <- cbind(row,row2)
class(train_vw)
head(train_vw)
dim(train_vw)[2]
for(i in 1:dim(train_vw)[2]){
train_vw_int <- paste0(train_vw[,i],train_vw_int)
}
train_vw_int <- matrix
train_vw_int <- matrix()
train_vw_int
for(i in 1:dim(train_vw)[2]){
train_vw_int <- paste0(train_vw[,i],train_vw_int)
}
head(train_vw_int)
train_vw_int <- matrix(nrow = nrow(train_vw),ncol = 1,dimnames = c(NA,NA))
train_vw_int <- matrix(nrow = nrow(train_vw),ncol = 1,dimnames = list(NA,NA))
train_vw_int <- matrix(nrow = nrow(train_vw),ncol = 1)
train_vw_int
for(i in 1:dim(train_vw)[2]){
train_vw_int <- paste0(train_vw_int,train_vw[,i],)
}
for(i in 1:dim(train_vw)[2]){
train_vw_int[,1] <- paste0(train_vw_int[,1],train_vw[,i],)
}
train_vw_int <- matrix(nrow = nrow(train_vw),ncol = 1)
for(i in 1:dim(train_vw)[2]){
train_vw_int[,1] <- paste0(train_vw_int[,1],train_vw[,i])
}
head(train_vw_int)
head(train_vw_int)
train_vw_int <- matrix(data = "",nrow = nrow(train_vw),ncol = 1)
train_vw_int
rm(list=ls());gc()
load(file='data/raw_data_log.RData')
head(train,1)
train[,95]<-as.factor(train[,95])
levels(train[,95]) <- c(1:9)
col <- names(train)
row <- paste0(train[,95],paste0(" '",train[,1]),"|")
row2 <- sapply(2:94, function(i) {
if(i==94){
paste0(col[i],":",train[,i])
}else{
paste0(col[i],":",train[,i]," ")
}
})
train_vw <- cbind(row,row2)
train_vw_int <- matrix(data = "",nrow = nrow(train_vw),ncol = 1)
for(i in 1:dim(train_vw)[2]){
train_vw_int[,1] <- paste0(train_vw_int[,1],train_vw[,i])
}
write.table(train_vw_int, "data/train.vw", sep="\t")
head(train_vw_int)
write.table(train_vw_int, "../train.vw", sep="\t", quote = F,row.names = F,col.names = F)
write.table(train_vw_int, "../train.vw", quote = F,row.names = F,col.names = F)
head(train)
rm(list=ls());gc()
load(file='data/raw_data_log.RData')
head(train,1)
train[,95]<-as.factor(train[,95])
levels(train[,95]) <- c(1:9)
col <- names(train)
row <- paste0(train[,95],paste0(" '",train[,1]),"|n ")
row2 <- sapply(2:94, function(i) {
if(i==94){
paste0(col[i],":",train[,i])
}else{
paste0(col[i],":",train[,i]," ")
}
})
train_vw <- cbind(row,row2)
train_vw_int <- matrix(data = "",nrow = nrow(train_vw),ncol = 1)
for(i in 1:dim(train_vw)[2]){
train_vw_int[,1] <- paste0(train_vw_int[,1],train_vw[,i])
}
write.table(train_vw_int, "../train.vw", quote = F,row.names = F,col.names = F)
head(test,1)
col <- names(test)
row <- paste0(1,paste0(" '",test[,1]),"|n ")
head(row)
head(col)
dim(col)
length(col)
head(test,1)
col <- names(test)
row <- paste0(1,paste0(" '",test[,1]),"|n ")
row2 <- sapply(2:94, function(i) {
if(i==94){
paste0(col[i],":",test[,i])
}else{
paste0(col[i],":",test[,i]," ")
}
})
test_vw <- cbind(row,row2)
test_vw_int <- matrix(data = "",nrow = nrow(test_vw),ncol = 1)
for(i in 1:dim(test_vw)[2]){
test_vw_int[,1] <- paste0(test_vw_int[,1],test_vw[,i])
}
write.table(train_vw_int, "../test.vw", quote = F,row.names = F,col.names = F)
write.table(test_vw_int, "../test.vw", quote = F,row.names = F,col.names = F)
head(test)
